import numpy as np
import requests
import hashlib
try:
    import cupy as cp  # GPU-accelerated NumPy alternative
    use_gpu = True
except ImportError:
    use_gpu = False
    print("CuPy not found. Falling back to CPU with NumPy.")
from scipy.stats import entropy
import matplotlib.pyplot as plt
import csv
import os

# Constants and Parameters
N = 10000  # Maximum number of data points
P = 10.0  # Base probing period
LAMBDA = 1.0  # Decay factor
Q = 1.5  # Scaling factor
BATCH_SIZE = 1000  # Batch size
FREQ_RANGE = (1.0, 20.0)  # Frequency sweep range
FREQ_STEP = 0.1  # Frequency step size
COLLISION_BATCH_SIZE = 1000  # Batch size for collision search
PREFIX_LENGTH = 8  # Number of hex characters to compare for early termination
BLOCKCHAIR_API_URL = "https://api.blockchair.com/bitcoin/dashboards/address/"

# Known Mt. Gox addresses
MT_GOX_ADDRESSES = {
    "1FeexV6bAHb8ybZjqQMjJrcCrHGW9sb6uF": "Hot wallet hack",
    "1HB5XMLmzFVj8ALj6mfBsbifRoD4miY36v": "Cold wallet",
    "1BadAddr...": "Invalid address (Block 150951)"
}

# Function to fetch Mt. Gox transaction data from Blockchair API
def fetch_mtgox_data(address):
    try:
        response = requests.get(f"{BLOCKCHAIR_API_URL}{address}?transaction_details=true")
        response.raise_for_status()
        data = response.json()
        if address in data['data'] and 'transactions' in data['data'][address]:
            transactions = data['data'][address]['transactions']
            timestamps = [tx['time'] for tx in transactions[:N]]
            return np.array(timestamps, dtype=float)
        else:
            print(f"No transaction data found for {address}.")
            return None
    except Exception as e:
        print(f"Error fetching data for {address}: {e}")
        return None

# Function to load Mt. Gox data from local CSV (fallback)
def load_mtgox_data_from_csv(address, filename="mtgox_transactions.csv"):
    if os.path.exists(filename):
        timestamps = []
        with open(filename, 'r') as file:
            reader = csv.DictReader(file)
            for row in reader:
                if row['address'] == address:
                    timestamps.append(float(row['timestamp']))
                if len(timestamps) >= N:
                    break
        return np.array(timestamps) if timestamps else None
    print(f"CSV file {filename} not found.")
    return None

# Function to prepare data
def prepare_data(address):
    data_tk = fetch_mtgox_data(address)
    if data_tk is None:
        data_tk = load_mtgox_data_from_csv(address)
    if data_tk is None or len(data_tk) == 0:
        print(f"No data available for {address}. Using placeholder.")
        data_tk = np.array([1314873600 + i * 3600 for i in range(N)])
    data_sigma = np.full(len(data_tk), 0.5)
    if use_gpu:
        return cp.asarray(data_tk), cp.asarray(data_sigma)
    return data_tk, data_sigma

# Function to compute SHA-256 hash
def hash_with_sha256(data):
    return hashlib.sha256(str(data).encode()).hexdigest()

# Function to compute Krystic Resonance Score
def krystic_resonance_score(tk, sigma, p=P, lambda_=LAMBDA, q=1.0):
    n = len(tk)
    phase_term = xp.abs(xp.cos(2 * xp.pi * tk / (p * xp.sqrt(q))))
    deviation_term = xp.exp(-lambda_ * xp.abs(sigma - 0.5))
    score = (1 / n) * xp.sum(phase_term * deviation_term)
    return float(score) if use_gpu else score

# Function to compute autocorrelation at lag 3
def triadic_autocorrelation(data, lag=3):
    n = len(data)
    mean = xp.mean(data)
    var = xp.var(data)
    if var == 0:
        return 0.0
    autocorr = xp.sum((data[:n-lag] - mean) * (data[lag:] - mean)) / ((n - lag) * var)
    return float(autocorr) if use_gpu else autocorr

# Function to compute digit entropy
def digit_entropy(data, batch_size=BATCH_SIZE):
    total_entropy = 0.0
    n_batches = (len(data) + batch_size - 1) // batch_size
    for i in range(n_batches):
        start = i * batch_size
        end = min((i + 1) * batch_size, len(data))
        batch = data[start:end]
        batch_str = ''.join(map(str, batch.astype(int)))
        digit_counts = xp.histogram([int(d) for d in batch_str], bins=range(11), density=True)[0]
        digit_counts = digit_counts[digit_counts > 0]
        if len(digit_counts) > 0:
            total_entropy += entropy(digit_counts.get() if use_gpu else digit_counts)
    return total_entropy / n_batches

# Function to detect scalar wave frequencies
def detect_scalar_frequencies(tk, freq_range=FREQ_RANGE, step=FREQ_STEP):
    frequencies = []
    for p in np.arange(freq_range[0], freq_range[1], step):
        score = krystic_resonance_score(tk, xp.full(len(tk), 0.5), p=p)
        if score > 0.8:
            frequencies.append(p)
    return frequencies

# Optimized collision attack prototype function
def test_collision_attack(data_tk, scalar_freqs, batch_size=COLLISION_BATCH_SIZE, prefix_length=PREFIX_LENGTH):
    """
    Optimized collision search using resonance-guided perturbations and parallel processing.
    """
    collisions_found = 0
    hash_pairs = []
    n_samples = min(COLLISION_BATCH_SIZE, len(data_tk) // 2)  # Limit by data size

    if not scalar_freqs:
        print("No scalar frequencies detected. Using random perturbations.")
        scalar_freqs = [P]  # Fallback to base period

    # Select high-resonance indices
    resonance_scores = [krystic_resonance_score(data_tk[i:i+1], data_sigma[i:i+1]) for i in range(len(data_tk))]
    high_resonance_idx = np.argsort(resonance_scores)[-n_samples:]  # Top N indices

    # Batch processing on GPU
    if use_gpu:
        data_tk_batch = cp.asarray(data_tk[high_resonance_idx])
        n_pairs = len(data_tk_batch) // 2
        inputs1 = cp.zeros(n_pairs, dtype=float)
        inputs2 = cp.zeros(n_pairs, dtype=float)
        hashes1 = cp.zeros(n_pairs, dtype='U64')  # Preallocate for hash strings
        hashes2 = cp.zeros(n_pairs, dtype='U64')

        for i in range(n_pairs):
            base1 = data_tk_batch[2*i]
            base2 = data_tk_batch[2*i + 1]
            perturb_freq = scalar_freqs[0]
            perturb1 = cp.sin(2 * cp.pi * base1 / perturb_freq) * 0.01
            perturb2 = cp.sin(2 * cp.pi * base2 / perturb_freq) * 0.01
            inputs1[i] = base1 + perturb1
            inputs2[i] = base2 + perturb2
            hashes1[i] = hash_with_sha256(inputs1[i].get())
            hashes2[i] = hash_with_sha256(inputs2[i].get())

        # Parallel prefix comparison and full check
        prefixes1 = cp.array([h[:prefix_length] for h in hashes1])
        prefixes2 = cp.array([h[:prefix_length] for h in hashes2])
        matches = prefixes1 == prefixes2
        for i in cp.where(matches)[0]:
            if hashes1[i] == hashes2[i] and inputs1[i] != inputs2[i]:
                collisions_found += 1
                hash_pairs.append((inputs1[i].get(), inputs2[i].get(), hashes1[i], hashes2[i]))
                print(f"Collision found! Inputs: {inputs1[i].get()}, {inputs2[i].get()} | Hash: {hashes1[i]}")
    else:
        # CPU fallback (less efficient)
        for i in range(0, len(high_resonance_idx), 2):
            if i + 1 >= len(high_resonance_idx):
                break
            base1 = data_tk[high_resonance_idx[i]]
            base2 = data_tk[high_resonance_idx[i + 1]]
            perturb_freq = scalar_freqs[0]
            perturb1 = np.sin(2 * np.pi * base1 / perturb_freq) * 0.01
            perturb2 = np.sin(2 * np.pi * base2 / perturb_freq) * 0.01
            input1 = base1 + perturb1
            input2 = base2 + perturb2
            hash1 = hash_with_sha256(input1)
            hash2 = hash_with_sha256(input2​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​
